---
title: "MC1RSY"
author: "Tobias Lauber, Haris Alic"
date: "2023-10-05"
output: html_document
---
# 1. Load librarys
```{r setup, include=FALSE}
library(tidyverse)
library(recommenderlab)
library(lsa) # might need for cosine function
library(dplyr)
library(tidyr) # for plot
```

für Kapitel in Outline schauen. Kapitel starten mit Hash. Unterkapitel mit hashhash. unter unter hashhashhash

# 2. load data
```{r cars}
data("MovieLense")
str(MovieLense)
```
## 2.1 Own testing
```{r}
summary(MovieLense)
```
type:
```{r}
class(MovieLense)
```
see it as data frame
```{r}
# Convert the realRatingMatrix to a data frame
movie_data <- as(MovieLense, "data.frame")

# View the first few rows of the data frame
head(movie_data, 20)
```
```{r}
column_names <- colnames(MovieLense)
# print(column_names)
```


# 6.1 Explorative Datenanalyse [10 Punkte]
1. Welches sind die am häufigsten geschauten Genres/Filme?
```{r}
# we look at slotnames
slotNames(MovieLense)
```

```{r}
# classes
class(MovieLense@data)
```
**We now look at all the unique vector ratings**
```{r}
vector_ratings <- as.vector(MovieLense@data)
unique(vector_ratings)
```
A rating of 0 indicates a missing rating, so we have to remove it
```{r}
vector_ratings <- vector_ratings[vector_ratings != 0]
```

we now look, how often a movie was watched, with the help of a dataframe

```{r}
views_per_movie <- colCounts(MovieLense)

dfrat <- data.frame(
  movie = names(views_per_movie),
  views = views_per_movie
)
dfviews <- dfrat[order(dfrat$views, decreasing = TRUE), ]
head(dfviews)
```
here we have our 10 most watched movies as a plot

```{r}
df <- as(MovieLense, "data.frame")
dfMeta <- as(MovieLenseMeta, "data.frame")

movieCount <- df %>%
  group_by(item) %>%
  summarize(freq = n()) %>%
  arrange(desc(freq))

ggplot(
  head(movieCount, 10),
  aes(x = reorder(item, -freq), y = freq)
) +
  geom_bar(stat = "identity") +
  xlab("Movie Titles") +
  ylab("Frequency") +
  ggtitle("Top 10 Most Frequent Movies") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

We see the top 10 most frequent watched movies, further below we investigate which genres are the most frequent watched.

Genres are in movielensemeta data.
1 means the genre is represent in the movie.
0 means it is not.
By using that logic, we can summarise the columns, and the result is the number of occurenses of the genres in movielense.
```{r}
# # Create new df and remove useless columns
# popular_genresdf <- MovieLenseMeta %>% select(-c("title", "year", "url"))

# # count the number of columns
# views_per_genre <- popular_genresdf %>%
#   summarise(across(everything(), sum)) %>%
#   arrange(desc(.))

# views_per_genre
```
```{r}
# Convert data to the pivot long format
# views_per_genre_long <- views_per_genre %>%
#   pivot_longer(everything(), names_to = "Column", values_to = "Sum")

# Count the frequency of each genre and create a new data frame
merged <- left_join(df, dfMeta, by = c("item" = "title"))
# Summarize by Genre
dfMetaGenres <- select(dfMeta, -(1:3))
# sum all genre columns
genreCount <- colSums(dfMetaGenres, na.rm = TRUE)
# create df
genreCountDf <- data.frame(genre = names(genreCount), freq = genreCount)
# Create a bar plot
ggplot(
  genreCountDf,
  aes(x = reorder(genre, -freq), y = freq)
) +
  geom_bar(stat = "identity") +
  xlab("Genres") +
  ylab("Total Frequency") +
  ggtitle("Most Frequently Watched Genres") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The plot shows the most frequently watches genres in descending order.
There is a clear imbalance in the genres, which can lead to a popularity bias, say the tendency to recommend popular items.
Which in turn can lead to even more people watching dramas and reinforce the bias.
An additional problem is that the same movie can be categorized in multiple genres, 
which can be misleading for the interpretation of the data.

## 6.1  
2. Wie verteilen sich die Nutzerratings der Filme gesamthaft bzw. nach Genres?

```{r}
MovieLenseMeta
```

```{r}
colnames(movie_data)
```


```{r}
movie_data
```

We need to join the 2 dataframes.
The only thing they have in common are the movie titles.
```{r}
# Grouped by Genre 1 plot
# Merge the data frames based on movie titles. We will further use this
merged_data <- left_join(movie_data, MovieLenseMeta,
  by = c("item" = "title")
)

# Create a new data frame with genres and ratings
genre_ratings <- merged_data %>%
  select(Action:Western, rating) %>%
  gather(genre, is_genre, Action:Western) %>%
  filter(is_genre == 1) %>%
  group_by(genre) %>%
  summarize(
    avg_rating = mean(rating),
    median_rating = median(rating)
  )

ggplot(genre_ratings, aes(x = genre, y = avg_rating, fill = avg_rating)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(avg_rating, 2), vjust = -0.5), size = 3) +
  coord_cartesian(ylim = c(0, 5)) +
  xlab("Genre") +
  ylab("Average Rating") +
  ggtitle("Distribution of Ratings by Genre") +
  scale_fill_gradient(low = "black", high = "blue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# install.packages("testthat")
library(testthat)
# Grouped by Genre multiple plots
genreRatings <- data.frame()
mergedRatingsGenre <- left_join(df, dfMeta, by = c("item" = "title"))
genreCols <- names(dfMetaGenres)

for (genre in genreCols) {
  # Filter the data to only include ratings for movies that belong to the current genre
  filteredData <- mergedRatingsGenre[mergedRatingsGenre[genre] == 1, ]
  # Add ratings to a new data frame tagged by genre
  rows <- data.frame(rating = filteredData$rating, genre = genre)
  # Add new rows to result data frame
  genreRatings <- rbind(genreRatings, rows)
}
ggplot(
  genreRatings,
  aes(x = rating)
) +
  geom_histogram(binwidth = 0.5, fill = "grey", alpha = 0.7) +
  facet_wrap(~genre, scales = "free_y") +
  xlab("Rating") +
  ylab("Frequency") +
  ggtitle("Distribution of User Ratings by Genre") +
  theme_minimal()

# test stuff
test_that("There are more genreRatings because one movie can be specified as multiple genres", {
  expect_false(identical(nrow(genreRatings), nrow(df)))
})
```
The distribution of user ratings is overall positive, with the majority of users rating 3 or higher.
3, 4 and 5 dominate the ratings, but the problem with 3 is that there are users for whom a 3-star rating is a bad rating and for others it is a good rating.
We will see how to deal with this problem later.

## 6.1 
3. Wie verteilen sich die mittleren Ratings pro Film bzw. pro Nutzer*in?

```{r}
merged_data
```
Mittlere Rating Filme
```{r}
# Gruppiere nach Filme und berechne den Durchschnitt

average_ratings_per_movie <- merged_data %>%
  group_by(item) %>%
  summarize(mean_rating = mean(rating))

median_rating <- median(average_ratings_per_movie$mean_rating)
min_rating <- min(average_ratings_per_movie$mean_rating)
max_rating <- max(average_ratings_per_movie$mean_rating)
mean_rating <- mean(average_ratings_per_movie$mean_rating)

# Resultate
cat("Median Filmbewertung:", median_rating, "\n")
cat("Minimale Filmbewertung:", min_rating, "\n")
cat("Maximale Filmbewertung:", max_rating, "\n")
cat("Durchschnittliche Filmbewertung:", mean_rating, "\n")
```

Plot Mittlere Rating Filme
```{r}
average_ratings_per_movie <- merged_data %>%
  group_by(item) %>%
  summarize(mean_rating = mean(rating))

# Plot
ggplot(data = average_ratings_per_movie, aes(x = mean_rating)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
  labs(
    title = "Durchschnittliche Ratings Filme",
    x = "Durchschnittliche Bewertung",
    y = "Nummer an Filmen"
  )
```

Mittlere Ratings User
```{r}
average_ratings_per_user <- merged_data %>%
  group_by(user) %>%
  summarize(mean_rating = mean(rating))

median_rating <- median(average_ratings_per_user$mean_rating)
min_rating <- min(average_ratings_per_user$mean_rating)
max_rating <- max(average_ratings_per_user$mean_rating)
mean_rating <- mean(average_ratings_per_user$mean_rating)

# Resultate
cat("Median durchschnittliche Userbewertung:", median_rating, "\n")
cat("Minimale durchschnittliche Userbewertung:", min_rating, "\n")
cat("Maximale durchschnittliche Userbewertung:", max_rating, "\n")
cat("Durchschnittliche durchschnittliche Userbewertung:", mean_rating, "\n")
```

Plot Mittlere Ratings User
```{r}
ggplot(data = average_ratings_per_user, aes(x = mean_rating)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
  labs(
    title = "Durchschnittliche Ratings User",
    x = "Durchschnittliche Bewertung",
    y = "Nummer an Usern"
  )
```

- Plot Average Rating Movies
  - We see that more users give very bad ratings than very good ratings.
    We assume these are hate ratings.
  
- Plot Average Ratings User
  - The highest frequency for individual users is between 3.5 and 4.
    That means the average user tends to rate movies generally positive.

## 6.1 
4. Welchen Einfluss hat die Normierung der Ratings pro Nutzer*in auf die Verteilung der mittleren Nutzer-ratings?

```{r}
normalized_ratings <- merged_data %>%
  group_by(user) %>%
  mutate(normalized_rating = (rating - mean(rating)) / sd(rating))

median_normalized_rating <- median(normalized_ratings$normalized_rating)
min_normalized_rating <- min(normalized_ratings$normalized_rating)
max_normalized_rating <- max(normalized_ratings$normalized_rating)
mean_normalized_rating <- mean(normalized_ratings$normalized_rating)

# Resultate
cat("Median der normalisierten avg Userbewertungen:", median_normalized_rating, "\n")
cat("Minimale normalisierte avg Userbewertung:", min_normalized_rating, "\n")
cat("Maximale normalisierte avg Userbewertung:", max_normalized_rating, "\n")
cat("Durchschnittliche normalisierte avg Userbewertung:", mean_normalized_rating, "\n")
```

Plot the average user ratings. This time with a Z score normalization
```{r}
ggplot(data = normalized_ratings, aes(x = normalized_rating)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
  labs(
    title = "Durchschnittliche Ratings User",
    x = "Durchschnittliche Bewertung",
    y = "Nummer an Usern"
  )
```

```{r}
# Normalize the MovieLense data
Norm <- normalize(MovieLense)
dfNorm <- as(Norm, "data.frame")
head(dfNorm)

# Calculate average ratings per user
avgNormRatingUser <- dfNorm %>%
  group_by(user) %>%
  summarize(avgRating = mean(rating)) %>%
  arrange(desc(avgRating))

# test that the mean is around zero
test_that("The mean is around zero", {
  expect_true(all.equal(mean(avgNormRatingUser$avgRating), 0, tolerance = 0.01))
})

# Visualize for a subset of users for non normalized data
df %>%
  filter(user %in% 1:12) %>%
  ggplot(aes(x = user, y = rating)) +
  geom_violin(color = "grey", fill = "grey", alpha = 0.5) +
  labs(
    x = "User",
    y = "Ratings",
    title = "Distribution of Ratings from Individual Users",
    subtitle = "Subset of Users 1-12"
  )

# Visualize for a subset of users for normalized data
dfNorm %>%
  filter(user %in% 1:12) %>%
  ggplot(aes(x = user, y = rating)) +
  geom_violin(color = "grey", fill = "grey", alpha = 0.5) +
  labs(
    x = "User",
    y = "Normalized Ratings",
    title = "Normalized Distribution of Ratings from Individual Users",
    subtitle = "Subset of Users 1-12"
  )
```

- Plot Average Ratings User
  We can see that normalization shifts the graph to the left
  -> The average of the user ratings is worse.

- Plot Violine
  Normalization eliminates user bias in collaborative filtering.
  Different users may have different rating standards, some may be more generous than others.
  Normalization centers each users rating around zero, making comparisons between users more meaningful,
  e.g. a normalized rating of zero would mean that the user finds the object average by their own standards.
  Before normalization, we do not see a clear difference between users rating preferences,
  while after normalization we see the similarities between users and their rating preferences, whether negative or positive.

## 6.1 

5. Welche strukturellen Charakteristika und Auffälligkeiten zeigt die User-Item Matrix?

```{r}
# install.packages("viridis")
library(viridis)
# Set a random seed for reproducibility
set.seed(42)

smallM <- MovieLense[
  sample(nrow(MovieLense), 50), sample(ncol(MovieLense), 50)
]
# Visualize the sparsity pattern of the smallMovieLense matrix
library(Matrix)
image(as(smallM, "matrix"), main = "Sparsity Pattern of User-Item Matrix", xlab = "Items", ylab = "Users", col = viridis(5))
legend("topright", legend = c("1", "2", "3", "4", "5"), fill = viridis(5), title = "Rating")

# Calculate sparsity level
movieMatrix <- as(MovieLense, "matrix")
totalN <- length(movieMatrix)
filledN <- sum(
  !is.na(movieMatrix) & movieMatrix > 0,
  na.rm = TRUE
)
sparsityLevel <- (totalN - filledN) / totalN
print(paste("Sparsity Level: ", round(sparsityLevel * 100, 2), "%"))
```

The user item matrix represents the interactions or ratings between users and items.
In the image the colored dots represent interactions or in our case ratings.
The large number of empty space indicates that users have not interacted with any item.
This is a common problem in real world user-item matrices,
as not every user interacts with every item.
The sparsity level of the item-user-matrix for the movielens dataset is ca. 93.7%.

# 6.2 Datenreduktion [6 Punkte]
Aufgabe 2: Reduziere den MovieLens Datensatz auf rund 400 Nutzerinnen und 700 Filme, indem du Filme und Nutzerinnen mit sehr wenigen Ratings entfernst.
1. Anzahl Filme und Nutzer*innen sowie Sparsity vor und nach Datenreduktion

```{r}
# Filtere 400 aktivste users
top_400_users <- merged_data %>%
  group_by(user) %>%
  summarize(total_ratings = n()) %>%
  arrange(desc(total_ratings)) %>%
  slice(1:400) %>%
  select(user)


# Filter the top 700 movies
top_movies <- merged_data %>%
  group_by(item) %>%
  summarize(total_ratings = n()) %>%
  arrange(desc(total_ratings)) %>%
  slice(1:700) %>%
  select(item)

# Reduziere dataset für Filme und User
dataFrame1 <- merged_data %>%
  filter(user %in% top_400_users$user, item %in% top_movies$item)
```
For 400

Before
```{r}
num_users_before <- length(unique(merged_data$user))
num_movies_before <- length(unique(merged_data$item))
sparsity_before <- 1 - (nrow(merged_data) / (num_users_before * num_movies_before))

# Output before reduction
cat("Number of users before reduction:", num_users_before, "\n")
cat("Number of movies before reduction:", num_movies_before, "\n")
cat("Sparsity before reduction:", sparsity_before, "\n")
```

After reduction
```{r}
num_users_after <- length(unique(dataFrame1$user))
num_movies_after <- length(unique(dataFrame1$item))
sparsity_after <- 1 - (nrow(dataFrame1) / (num_users_after * num_movies_after))

# Output after reduction
cat("Number of users after reduction:", num_users_after, "\n")
cat("Number of movies after reduction:", num_movies_after, "\n")
cat("Sparsity after reduction:", sparsity_after, "\n")
```
That makes sense, because if we kick out the users who watch fewer movies, then the sparsity should decrease.

Plot Avarage ratings.
```{r}
library(gridExtra)

avg_ratings_400 <- dataFrame1 %>%
  group_by(item) %>%
  summarize(mean_rating = mean(rating))

avg_ratings_full <- merged_data %>%
  group_by(item) %>%
  summarize(mean_rating = mean(rating))

avg400 <- ggplot(data = avg_ratings_400, aes(x = mean_rating)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
  labs(
    title = "400er Reduktion",
    x = "Durchschnittliche Bewertung",
    y = "Anzahl Filme"
  )

avgfull <- ggplot(data = avg_ratings_full, aes(x = mean_rating)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
  labs(
    title = "voller Datensatz",
    x = "Durchschnittliche Bewertung",
    y = "Anzahl Filme"
  )

grid.arrange(avg400, avgfull, ncol = 2)
```

Second dataframe with 400 best users sliced between 200 and 600.
```{r}
top_600_users <- merged_data %>%
  group_by(user) %>%
  summarize(total_ratings = n()) %>%
  arrange(desc(total_ratings)) %>%
  slice(201:600) %>%
  select(user)

# Filter the top 700 movies
top_movies <- merged_data %>%
  group_by(item) %>%
  summarize(total_ratings = n()) %>%
  arrange(desc(total_ratings)) %>%
  slice(1:700) %>%
  select(item)

# Reduziere dataset für Filme und User
dataFrame2 <- merged_data %>%
  filter(user %in% top_600_users$user, item %in% top_movies$item)
```

Plot average ratings for 600
```{r}
avg_ratings_600 <- dataFrame2 %>%
  group_by(item) %>%
  summarize(mean_rating = mean(rating))

avg_ratings_full <- merged_data %>%
  group_by(item) %>%
  summarize(mean_rating = mean(rating))

avg600 <- ggplot(data = avg_ratings_600, aes(x = mean_rating)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
  labs(
    title = "600er Reduktion",
    x = "Durchschnittliche Bewertung",
    y = "Anzahl Filme"
  )

avgfull <- ggplot(data = avg_ratings_full, aes(x = mean_rating)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
  labs(
    title = "voller Datensatz",
    x = "Durchschnittliche Bewertung",
    y = "Anzahl Filme"
  )

grid.arrange(avg600, avgfull, ncol = 2)
```

Again, before data reduction
```{r}
num_users_before <- length(unique(merged_data$user))
num_movies_before <- length(unique(merged_data$item))
sparsity_before <- 1 - (nrow(merged_data) / (num_users_before * num_movies_before))

# Output before reduction
cat("Number of users before reduction:", num_users_before, "\n")
cat("Number of movies before reduction:", num_movies_before, "\n")
cat("Sparsity before reduction:", sparsity_before, "\n")
```
This is after you have reduced to the 600 most relevant users, without the 200 most relevant users.

```{r}
num_users_after <- length(unique(dataFrame2$user))
num_movies_after <- length(unique(dataFrame2$item))
sparsity_after <- 1 - (nrow(dataFrame2) / (num_users_after * num_movies_after))

# Output after reduction
cat("Number of users after reduction:", num_users_after, "\n")
cat("Number of movies after reduction:", num_movies_after, "\n")
cat("Sparsity after reduction:", sparsity_after, "\n")
```

The sparsity with 943 users and 1664 films is 0.93, which is very high.
We reduce the data set to 400 users (who gave the most ratings) and 700 movies.
We see that the sparsity has dropped to 76 percent, which is reasonable,
because we have the most active users and most rated movies left.
The second reduced data frame takes the most active 600 users, but cuts out the first 200.
We see that the sparsity increases to 88 percent.
The first 200 most active users are no longer present, so we have a higher sparsity.

## 6.2
3. Zusatz für Gruppen: Quantifiziere die “Intersection over Union” aller reduzierten Datensätze paarweise.

```{r}
head(dataFrame1)
```
Die Formel ist: 

IOU = (# gemeinsame Filme) / (#Filme in beiden df zusammen )

```{r}
# Distinkte Movies / User für die Beiden Dataframes
anzahl_movies_400 <- n_distinct(dataFrame1$item)
anzahl_users_400 <- n_distinct(dataFrame1$user)

anzahl_movies_600 <- n_distinct(dataFrame2$item)
anzahl_users_600 <- n_distinct(dataFrame2$user)

# Intersection
common_movies <- intersect(dataFrame1$item, dataFrame2$item)
common_users <- intersect(dataFrame1$user, dataFrame2$user)

# IOU
IOU_movies <- length(common_movies) / (anzahl_movies_400 + anzahl_movies_600 - length(common_movies))

IOU_users <- length(common_users) / (anzahl_users_400 +
  anzahl_users_600 - length(common_users))

cat("Intersection over Union für Filme:", IOU_movies, "\n")
cat("Intersection over Union für Nutzer:", IOU_users, "\n")
```
This result was logical after our data separation, as we use the same films and 0.333 also makes sense,
as only users 201-400 appear in both data frames.
The intersection over union is a measure of the overlap between two data sets.
For the user IoU, there is an overlap of about 33% between the first and second data sets.
For the movies IoU, the overlap between the first and second data sets is about 100%.
The overlap is quite reasonable and therefore the data sets are not too similar.
We could reduce the similarity by choosing other splitting techniques, e.g. instead of 400 users we could have 700,
and the indexing could range from 1 to 500 and from 201 to 700 and see how the IoU behaves.

# 6.3 Analyse Ähnlichkeitsmatrix [12 Punkte]

Aufgabe 3: Erzeuge einen IBCF Recommender und analysiere die Ähnlichkeitsmatrix des trainierten Modelles für den reduzierten Datensatz.
1. Zerlege den Datensatz in Trainings- und Testdaten im verhältnis 4:1.

```{r}
matrixReduced1 <- as(dataFrame1, "realRatingMatrix")

evalScheme1 <- evaluationScheme(
  matrixReduced1,
  method = "split",
  train = 0.8,
  given = 5,
  goodRating = 4
)
evalScheme1
```

```{r}
matrixReduced2 <- as(dataFrame2, "realRatingMatrix")

evalScheme2 <- evaluationScheme(
  matrixReduced2,
  method = "split",
  train = 0.8,
  given = 5,
  goodRating = 4
)
evalScheme2
```

We use the evaluationScheme for the first data set with the 400 most active users,
and we decided that good ratings are equal to 4 because we thought 3 was too mediocre.
The output is a realRatingMatrix with 67427 ratings and the training set proportion is 0.8.
For the second dataset, we have the same parameters and there are 33300 ratings.

1. Trainiere ein IBCF Modell mit 30 Nachbarn und Cosine Similarity.

```{r}
# Get the training set from the evaluationScheme object
trainData1 <- getData(evalScheme1, "train")
testData1 <- getData(evalScheme1, "known")

trainData2 <- getData(evalScheme2, "train")
testData2 <- getData(evalScheme2, "known")

# Train the IBCF model
trainedModel1 <- Recommender(
  trainData1,
  method = "IBCF", param = list(k = 30, method = "Cosine")
)
trainedModel2 <- Recommender(
  trainData2,
  method = "IBCF", param = list(k = 30, method = "Cosine")
)
```

Top 400
```{r}
sim_400 <- getModel(trainedModel1)$sim
image(getModel(trainedModel1)$sim,
  main = "IBCF Similarity Matrix Heatmap for top 400"
)
```

Top 600 without first 200
```{r}
sim_600 <- getModel(trainedModel2)$sim
image(getModel(trainedModel2)$sim,
  main = "IBCF Similarity Matrix Heatmap for top 600 (without first 200)"
)
```

The first diagram entitled "Top 600 (excluding the first 200) similarity matrix heatmap," 
shows an unevenly distributed heatmap and appears quite sparse.
The second chart, titled "IBCF Similarity Matrix Heatmap for top 400," shows a slightly denser heatmap,
which suggests that a greater number of similarities may have been identified between the articles.
The difference in active users can be derived from the density of interactions.
If a model has more active users, we would expect a denser similarity matrix as there are more reviews and interactions from which similarities can be derived.
interactions from which similarities can be calculated.
The heatmap with more dots (or less white space) could be the one with more active users, assuming that "active
refers to the frequency or volume of ratings given by users. More active users usually lead to a
denser similarity matrix as there are more data points to calculate the similarities between each item

## 6.3 
3. Bestimme die Filme, die am häufigsten in der Cosine-Ähnlichkeitsmatrix auftauchen und analyisiere Vorkommen und Ratings im reduzierten Datensatz.
generelle Verteilung der Anzahl an 'Auftauchen' in der Cosine Ähnlichkeitsmatrix

```{r}
sums400 <- colSums(sim_400)
twentytop_400 <- head(sort(sums400, decreasing = TRUE), 20)
hist(
  sums400,
  breaks = 50, main = "Distribution of Movie Rating Occurences",
  xlab = "Amount of Times Rated",
  ylab = "Number of Movies that occur x times"
)
```
We can see that a large proportion of movies do not appear at all or only very rarely (left side).
The largest proportion of movies rated is between 0 and 50 occurrences.
We conclude that most movies are rated very rarely.

```{r}
sumsDF2 <- colSums(sim_600)
twentytop_600 <- head(sort(sumsDF2, decreasing = TRUE), 20)
hist(
  sumsDF2,
  breaks = 50, main = "Distribution of Movie Rating Occurences",
  xlab = "Amount of Times Rated",
  ylab = "Number of Movies that occur x times"
)
```
Because the users from the second dataset are less active than the users from the first dataset, 
we see more movies are rated fewer times.

```{r}
twentytop_400
```
These are the top 20 most rated movies for the first data set.

```{r}
twentytop_600
```
These are the top 20 most rated movies for the second data set.
It is interesting that the film 'Fallen (1998)', which appeared the most in the
data set 400, does not even make the top20 here.

First Data Set
```{r}
ggplot() +
  geom_histogram(data = dataFrame1 %>% group_by(item) %>%
    count(), aes(n), binwidth = 0.05, color = "blue", fill = "white", alpha = 0.5) +
  labs(
    title = "Comparison between Recommendation and Reduced Data Set 1",
    subtitle = "Top 20 Movies from IBCF in green",
    x = "Amount of Ratings",
    y = "Amount of Movies",
  ) +
  geom_vline(xintercept = twentytop_400, color = "green")
```
The blue histogram shows the number of ratings per movie for the first data set.
The green lines show the 20 most rated movies from the IBCF.
We see that the recommendations tend to be in the middle of the distribution.
The model has not choosed the most rated movies.

Second Data Set
```{r}
ggplot() +
  geom_histogram(data = dataFrame2 %>% group_by(item) %>%
    count(), aes(n), binwidth = 0.05, color = "blue", fill = "white", alpha = 0.5) +
  labs(
    title = "Comparison between Recommendation and Reduced Data Set 1",
    subtitle = "Top 20 Movies from IBCF in green",
    x = "Amount of Ratings",
    y = "Amount of Movies",
  ) +
  geom_vline(xintercept = twentytop_600, color = "green")
```
The blue histogram shows the number of ratings per film for the first data set.
The green lines show the 20 best-rated films from the IBCF.
It can be seen that the recommendations tend to lie in the middle of the distribution,
regardless of the skewness of the distribution.
The model has a balanced recommendation between rarely and frequently rated films.

Movie Ratings Distribution (Normalized) for top 20 (first data set)
```{r}
names_twentytop_400 <- names(twentytop_400)

top20simmrat <- dataFrame1 %>%
  group_by(item) %>%
  filter(item %in% names_twentytop_400)

ggplot() +
  geom_histogram(data = top20simmrat, aes(rating), binwidth = 0.1) +
  facet_wrap(vars(top20simmrat$item)) +
  labs(
    x = "Normalized Ratings",
    y = "Number of Ratings",
    title = "Movie Ratings Distribution (Normalized) for top 20 Movies"
  )
```
We see for each individual movie of our recommender (top 400) how it was rated, 
e.g. the "When We Were Kings (1996)" was rated very positively,
where as "Excess Baggage (1997)" was rated very negatively.

Movie Ratings Distribution (Normalized) for top 20 (second data set)
```{r}
names_twentytop_600 <- names(twentytop_600)

top20simmrat <- dataFrame2 %>%
  group_by(item) %>%
  filter(item %in% names_twentytop_600)

ggplot() +
  geom_histogram(data = top20simmrat, aes(rating), binwidth = 0.1) +
  facet_wrap(vars(top20simmrat$item)) +
  labs(
    x = "Normalized Ratings",
    y = "Number of Ratings",
    title = "Movie Ratings Distribution (Normalized) for top 20 Movies"
  )
```
The distribution is much sparser than before.
We can see for each individual movie of our recommender (second data set) how it was rated, 
In contrast to the other data set, this one tends to be rated more positively,
For example, the film "Short Cuts (1993)" was rated very positively,
whereas "Sirens (1994)" was rated rather negatively.

4. Wiederhole die Analyse, indem du bei der Datenpartitionierung die Anzahl nicht-maskierter
   Produkte der Test-User veränderst und kommentiere den Einfluss auf die Resultate.
```{r}
# Convert from df to realRatingMatrix
matrixReduced1 <- as(dataFrame1, "realRatingMatrix")

# Define the given values to iterate over
given_values <- c(5, 10, 20, 40)

# Iterate over the different given values
for (given in given_values) {
  cat("Processing for given =", given, "\n")

  # Create evaluation scheme
  evalScheme <- evaluationScheme(
    matrixReduced1,
    method = "split",
    train = 0.8,
    given = given,
    goodRating = 4
  )

  # Train the IBCF model
  trained_model <- Recommender(
    getData(evalScheme, "train"),
    method = "IBCF",
    param = list(k = 30, method = "Cosine")
  )

  # Extract the similarity matrix
  simMatrix <- getModel(trained_model)$sim

  # Find the top 10 most similar movies
  top10 <- apply(simMatrix, 1, function(x) {
    order(x, decreasing = TRUE)[1:10]
  })

  # Unlist and tabulate to find most frequent
  mostFrequentMovies <- table(as.vector(top10))
  # Sort
  mostFrequentMovies <- sort(mostFrequentMovies, decreasing = TRUE)

  # Process for ggplot2
  dfMostFrequentMovies <- as.data.frame(mostFrequentMovies)
  colnames(dfMostFrequentMovies) <- c("item", "freq")

  # Get the names for the movies for given ids in mostFreqentMovies
  dfMostFrequentMovies$MovieTitle <- dfMeta[as.numeric(names(mostFrequentMovies)), "title"]

  # Visualize
  p <- ggplot(
    head(dfMostFrequentMovies, 10),
    aes(x = reorder(MovieTitle, -freq), y = freq)
  ) +
    geom_bar(stat = "identity") +
    xlab("Movie Titles") +
    ylab("Frequency") +
    ggtitle(paste("Top 10 Most Frequent Movies in Similarity Matrix for given =", given)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  print(p)
  ggsave(paste("similarity_matrix_given_", given, ".png", sep = ""))
}
```

The IBCF Recommender uses a test set of user profiles that are not part of the training but of the predictions.
Additionally, there is a set of masked ratings to evaluate the prediction. For the masking, we use the Given-x parameter, which is a random selection 
from all test users for the prediction, the rest is part of the evaluation.
It turns out that the same result is not always achieved for different given values,
e.g. the recommendation results for Given = 5 differ from those of the others.
We can state the same for the second data set.

## 6.4
Aufgabe 4 (DIY): Implementiere Funktionen zur Berechnung von Ähnlichkeitsmatrizen bei IBCF Recommenders für 
(a) Cosine Similarity mit ordinale Ratings und 
(b) Jaccard Similarity mit binären Ratings

1. Vergleiche die Resultate beider Funktionen hinsichtlich Übereinstimmung und Laufzeit mit dem Resultat der Funktion Recommender()
   und der eines anderen R-Paketes anhand 100 zufällig gewählter Filme.

Jaccard Similarity Function
```{r}
getJaccardSim <- function(M) {
  A <- tcrossprod(M)
  im <- which(A > 0, arr.ind = TRUE)
  b <- rowSums(M)
  Aim <- A[im]
  sparseMatrix(
    i = im[, 1],
    j = im[, 2],
    x = Aim / (b[im[, 1]] + b[im[, 2]] - Aim),
    dims = dim(A)
  )
}
```

```{r}
set.seed(7)
index <- sort(sample(1:nrow(MovieLense), 100)) # wähle 100 zufällige Filme
oursample <- MovieLense[index]
# reguläre Werte Matrix
oursample_Matrix <- as(oursample, "matrix")
oursample_Matrix[is.na(oursample_Matrix)] <- 0 # ersetze na durch 0

# binäre Werte Matrix
oursample_bin <- binarize(oursample, 4)
sampledMatrix <- as(oursample_bin, "matrix")

jaccard_sim <- as(getJaccardSim(sampledMatrix), "matrix")

jaccard_sim[1:5, 1:5]
```

Cosinus Similarity Function
```{r}
getCosineSim <- function(M) {
  similarity <- M / sqrt(rowSums(M * M))
  similarity[is.na(similarity)] <- 0
  similarity <- similarity %*% t(similarity)
  similarity <- as(similarity, "matrix")
}
```

We use the cosine similarity on the regular data (100 movies)
```{r}
cosinus_sim <- getCosineSim(oursample_Matrix)
cosinus_sim[1:5, 1:5]
```

We compare the custom Cosine with Recommender Cosine and proxy Cosine
```{r}
realRM <- as(oursample_Matrix, "realRatingMatrix")
cosine_rec <- as.matrix(similarity(realRM, method = "cosine", which = "users"))
diag(cosine_rec) <- 1
rescale <- function(x) {
  return(1 / 2 * (x + 1))
}
co_sim_rec <- apply(cosinus_sim, 1, rescale)

max(abs(cosine_rec - co_sim_rec), na.rm = TRUE)

all.equal(cosine_rec, co_sim_rec)
```
Recommenderlab rescales the matrix, so we have to do it also.
Now we proof if our implementation is allmost equal to the recommenderlab implementation.
We see there is almost no difference (floating point error) between our custom implementation and the recommender cosine.

Now we compare our implementation with the proxy cosine
```{r}
library(proxy)
proxy_cosine <- as(cosine(t(oursample_Matrix)), "matrix")
max(abs(cosinus_sim - proxy_cosine))
all.equal(cosinus_sim, proxy_cosine)
```
We see that there is almost no difference between our custom implementation and the proxy cosine.

# Runtime
```{r}
startTime <- Sys.time()
cosineSimMatrix <- getCosineSim(oursample_Matrix)
endTime <- Sys.time()
runtimeCustom <- endTime - startTime
print("Runtim for custom cosine function:")
print(runtimeCustom)

startTime <- Sys.time()
realRM <- as(oursample_Matrix, "realRatingMatrix")
cosine_rec <- as.matrix(similarity(realRM, method = "cosine", which = "users"))
endTime <- Sys.time()
runtimeCustom <- endTime - startTime
print("Runtim for recommender cosine function:")
print(runtimeCustom)

startTime <- Sys.time()
proxy_cosine <- as(cosine(t(oursample_Matrix)), "matrix")
endTime <- Sys.time()
runtimeCustom <- endTime - startTime
print("Runtim for proxy cosine function:")
print(runtimeCustom)
```
The proxy cosine function is about four times slower than our custom implementation.
The recommender cosine function is also slower than our custom implementation,
because we have to convert the matrix into a realRatingMatrix, which requires more steps and is slower.
is slower.

1. Visualisiere und vergleiche die Verteilung der Ähnlichkeiten von Cosine Similarity für ordinale Ratings
und von Jaccard Similarity für binäre Ratings mittels den von dir implementierten Funktionen.
```{r}
cos_jacc <- mean(abs(cosinus_sim - jaccard_sim), na.rm = TRUE)
print("Mean Absolute Difference between Cosinus and Jaccard:")
print(cos_jacc)
```
Interpretation:
The Mean Absolute Difference (MAD) can be interpreted as the average error between the two measures.
Since cosine similarity and Jaccard similarity are different metrics
(cosine similarity takes into account the magnitude of the vectors, while Jaccard similarity is based on the presence/absence of characteristics),
some difference is to be expected. A MAD of 0.12 indicates that there is some discrepancy between the two measures,
the overall magnitude of this discrepancy is modest. This suggest that for the particular dataset,
both measures are providing relatively similar information.

Visualization
```{r}
library(ggplot2)
# install.packages("reshape2")
library(reshape2)

# Daten für die Heatmap vorbereiten
cosinus_melted <- melt(cosinus_sim)
jaccard_melted <- melt(jaccard_sim)

# Plotte cos Heatmap
movie_labels <- rownames(cosinus_sim)

# long format
cos_sim_df <- as.data.frame(as.table(cosinus_sim))
cos_sim_df$Var1 <- factor(cos_sim_df$Var1, levels = movie_labels)
cos_sim_df$Var2 <- factor(cos_sim_df$Var2, levels = movie_labels)

# heatmap
ggplot(data = cos_sim_df, aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(title = "Cosine Similarity Matrix", x = "Movies", y = "Movies") +
  scale_x_discrete(labels = rep("", nrow(cos_sim_df))) +
  scale_y_discrete(labels = rep("", nrow(cos_sim_df)))
```

```{r}
# jaccard heatmap
ggplot(data = jaccard_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  labs(title = "Jaccard Similarity Matrix", x = "Movies", y = "Movies") +
  theme_minimal()
```

Our dataset consists of ratings which are non binary and on different scales.
Cosine similarity (CS) can capture the nuances in user preferences more effectively than Jaccard Similarity (JS).
The CS will consider different levels of preference (e.g. rating scales from 1 to 5),
whereas Jaccard similarity only considers whether an item was rated or not, ignoring the rating scale.

Our users may rate only a small subset of all available items (high sparsity).
CS can still find a high degree of similarity between two users even if they have rated a different number of items.

Jaccard does not take into account the magnitude of the interaction and would consider two users who rated a different
number of items less similar, even if the smaller set of ratings is a subset of the larger set of ratings.

# 6.5 Produktabdeckung - Top-N Listen von IBCF und UBCF [12 Punkte]
Aufgabe 5: Vergleiche und diskutiere Top-N Empfehlungen von IBCF und UBCF Modellen mit 30 Nachbarn
und Cosine Similarity für den reduzierten Datensatz.
1. Berechne die Top-15 Empfehlungen aller Testnutzer*innen via IBCF und UBCF

```{r}
library(recommenderlab)

dataFrame1 <- merged_data %>%
  filter(user %in% top_400_users$user, item %in% top_movies$item)

matrixReduced1 <- as(dataFrame1, "realRatingMatrix")

evalScheme1 <- evaluationScheme(
  matrixReduced1,
  method = "split",
  train = 0.8,
  given = 5,
  goodRating = 4
)

trainData1 <- getData(evalScheme1, "train")
testData1 <- getData(evalScheme1, "known")

dataFrame2 <- merged_data %>%
  filter(user %in% top_600_users$user, item %in% top_movies$item)

matrixReduced2 <- as(dataFrame2, "realRatingMatrix")

evalScheme2 <- evaluationScheme(
  matrixReduced2,
  method = "split",
  train = 0.8,
  given = 5,
  goodRating = 4
)

trainData2 <- getData(evalScheme2, "train")
testData2 <- getData(evalScheme2, "known")

ibcfModel <- Recommender(trainData1, method = "IBCF", param = list(k = 30, method = "Cosine"))
ubcfModel <- Recommender(trainData1, method = "UBCF", param = list(nn = 30, method = "Cosine"))

# Function to get top N recommendations
getTopN <- function(model, data, n = 15) {
  sapply(seq(nrow(data)), function(i) {
    recs <- predict(model, newdata = data[i, ], n = n)
    as(recs, "list")[[1]]
  })
}

# Get top 15 recommendations for all users
top15_ibcf <- getTopN(ibcfModel, testData1)
top15_ubcf <- getTopN(ubcfModel, testData1)
```

2. Vergleiche die Top-15 Empfehlungen von IBCF vs UBCF für drei Testnutzer*innen mittels Tabelle

```{r}
comparison_table <- data.frame(
  # Annahme: Die Zeilen repräsentieren die Top-15 Empfehlungen
  IBCF_User1 = top15_ibcf[1],
  UBCF_User1 = top15_ubcf[1],
  IBCF_User2 = top15_ibcf[2],
  UBCF_User2 = top15_ubcf[2],
  IBCF_User3 = top15_ibcf[3],
  UBCF_User3 = top15_ubcf[3]
)
comparison_table

print("für user 1: ")
print(intersect(top15_ibcf[1], top15_ubcf[1]))
cat("\n")

print("für user 2: ")
print(intersect(top15_ibcf[2], top15_ubcf[2]))
cat("\n")

print("für user 3: ")
print(intersect(top15_ibcf[3], top15_ubcf[3]))
```
We do not see any intersect between the recommendations of the two models.

3. Visualisiere und diskutiere für alle Testnutzer*innen summarisch die 
   Verteilung der Top-15 Empfehlungen von IBCF und UBCF.
```{r}
# Assuming 'top15_ibcf' and 'top15_ubcf' are lists of character vectors containing movie titles
# Let's first combine all recommendations into a single vector for each method
all_ibcf <- unlist(top15_ibcf)
all_ubcf <- unlist(top15_ubcf)

# Create a table of frequencies for each set of recommendations
freq_ibcf <- table(all_ibcf)
freq_ubcf <- table(all_ubcf)

# Convert the tables to data frames for plotting
df_ibcf <- as.data.frame(freq_ibcf)
df_ubcf <- as.data.frame(freq_ubcf)

# Rename the columns for clarity
names(df_ibcf) <- c("Movie", "IBCF_Count")
names(df_ubcf) <- c("Movie", "UBCF_Count")

# Merge the two data frames by movie titles
df_combined <- merge(df_ibcf, df_ubcf, by = "Movie", all = TRUE)

# Replace NAs with zeros
df_combined[is.na(df_combined)] <- 0

# Now we can plot the comparison using ggplot2
library(ggplot2)

# Creating a melted version of the dataframe for ggplot2
df_melted <- reshape2::melt(df_combined, id.vars = "Movie")

# Plotting
ggplot(data = df_melted, aes(x = Movie, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Movie", y = "Recommendation Count", fill = "Method") +
  ggtitle("Distribution of Movie Recommendations: IBCF vs. UBCF") + 
  theme(axis.text.x = element_blank())
```
In the graph we see the distribution for both recommenders recommending different movies.
For example, UBCF recommends a movie very frequently (far left in the graph), while the IBCF recommendation program does not. On the far right,
we see a lot of movies that are only recommended by UBCF and not by IBCF.

UBCF algorithms make recommendations based on the similarity of user interaction patterns.
If a group of users has interacted with a certain set of movies that is not widely popular or rated by other users,
UBCF can still recommend these movies to similar users.

UBCF is typically better at capturing the "long tail" of less popular items.
This means it can recommend movies that have been rated by a small number of similar users,
while IBCF might miss these if the items themselves dont have enough interactions to establish strong item-item similarities.

# 6.6

Als Erinnerung, hier sind unsere Recommender aus Aufgabe 5
```{r}
# IBCF 15 Empfehlungen
recmod_ibcf_400 <- Recommender(trainData1, method = "IBCF", param = list(method = "Cosine", k = 30))

rec_ibcf_400 <- as(predict(recmod_ibcf_400, testData1, n = 15), "list")

# UBCF 15 Empfehlungen
recmod_ubcf_400 <- Recommender(trainData1, method = "UBCF", param = list(method = "Cosine", nn = 30))

rec_ubcf_400 <- as(predict(recmod_ubcf_400, testData1, n = 15), "list")
```

Nun können wir die Intersektion bon IBCF und UBCF für data_400 plotten.

Ordinales Rating
```{r}
list_ord <- unlist(names(rec_ubcf_400) %>% map(~ (sum(rec_ibcf_400[[.x]] %in%
  rec_ubcf_400[[.x]])) /
  15 * 100))

df_ord <- as.data.frame(list_ord)
# df_ord
```


```{r}
ggplot() +
  geom_histogram(data = df_ord, aes(list_ord), binwidth = 1) +
  labs(
    title = "Ordinal rating: Intersection IBCF/UBCF",
    x = "Intersection in %",
    y = "#Users"
  )
```

## noch nicht ganz fertig, weiss nicht ganz wie weiter
```{r}

```



# 6.7
Hier vergleichen wir IBCF vs SVD

Wir benutzen dabei die selbe vorgehensweise, wie bei aufgabe 6

```{r}
n <- 15

# ibcf
recmod_ibcf_400 <- Recommender(trainData1, method = "IBCF", param = list(method = "Cosine", k = 30))

rec_ibcf_400 <- as(predict(recmod_ibcf_400, testData1, n = n), "list")



# svd 10

recmod_svd_400_10 <- Recommender(trainData1, method = "SVD", param = list(k = 10))
rec_svd_400_10 <- as(predict(recmod_svd_400_10, testData1, n = n), "list")




# svd 20

recmod_svd_400_20 <- Recommender(trainData1, method = "SVD", param = list(k = 20))
rec_svd_400_20 <- as(predict(recmod_svd_400_20, testData1, n = n), "list")



# svd 30
recmod_svd_400_30 <- Recommender(trainData1, method = "SVD", param = list(k = 30))
rec_svd_400_30 <- as(predict(recmod_svd_400_30, testData1, n = n), "list")


# svd 40
recmod_svd_400_40 <- Recommender(trainData1, method = "SVD", param = list(k = 40))
rec_svd_400_40 <- as(predict(recmod_svd_400_40, testData1, n = n), "list")


# svd 50
recmod_svd_400_50 <- Recommender(trainData1, method = "SVD", param = list(k = 50))
rec_svd_400_50 <- as(predict(recmod_svd_400_50, testData1, n = n), "list")
```


```{r}
svd10reclist <- unlist(names(recmod_ibcf_400) %>% map(~ (sum(
  rec_ibcf_400[[.x]] %in% rec_svd_400_10[[.x]]
)) / 15 * 100))


svd20reclist <- unlist(names(recmod_ibcf_400) %>% map(~ (sum(
  rec_ibcf_400[[.x]] %in% rec_svd_400_20[[.x]]
)) / 15 * 100))

svd30reclist <- unlist(names(recmod_ibcf_400) %>% map(~ (sum(
  rec_ibcf_400[[.x]] %in% rec_svd_400_30[[.x]]
)) / 15 * 100))

svd40reclist <- unlist(names(recmod_ibcf_400) %>% map(~ (sum(
  rec_ibcf_400[[.x]] %in% rec_svd_400_40[[.x]]
)) / 15 * 100))

svd50reclist <- unlist(names(recmod_ibcf_400) %>% map(~ (sum(
  rec_ibcf_400[[.x]] %in% rec_svd_400_50[[.x]]
)) / 15 * 100))

# dataframes
df_svd_400_10 <- as.data.frame(svd10reclist)
df_svd_400_20 <- as.data.frame(svd10reclist)
df_svd_400_30 <- as.data.frame(svd10reclist)
df_svd_400_40 <- as.data.frame(svd10reclist)
df_svd_400_50 <- as.data.frame(svd10reclist)
```

```{r}
# better version of above
# Funktion für die Berechnung der Überschneidung
calculate_overlap <- function(rec_ibcf, rec_svd, n) {
  names(rec_ibcf) %>% map(~ sum(rec_ibcf[[.x]] %in% rec_svd[[.x]]) / n * 100)
}

# Berechnung der Überschneidung für verschiedene SVD-Konfigurationen
svd10reclist <- calculate_overlap(rec_ibcf_400, rec_svd_400_10, 15)
svd20reclist <- calculate_overlap(rec_ibcf_400, rec_svd_400_20, 15)
svd30reclist <- calculate_overlap(rec_ibcf_400, rec_svd_400_30, 15)
svd40reclist <- calculate_overlap(rec_ibcf_400, rec_svd_400_40, 15)
svd50reclist <- calculate_overlap(rec_ibcf_400, rec_svd_400_50, 15)

# Erstellung von DataFrames
df_svd_400_10 <- as.data.frame(svd10reclist)
df_svd_400_20 <- as.data.frame(svd20reclist)
df_svd_400_30 <- as.data.frame(svd30reclist)
df_svd_400_40 <- as.data.frame(svd40reclist)
df_svd_400_50 <- as.data.frame(svd50reclist)
```


```{r}
# einschub, sollte so funktionieren

# Create a function to calculate overlap
calculate_overlap <- function(rec_ibcf, rec_svd, n, model_name) {
  data.frame(
    val = names(rec_ibcf) %>%
      map_dbl(~ sum(rec_ibcf[[.x]] %in% rec_svd[[.x]]) / n * 100),
    model = model_name
  )
}

# Calculate overlap for different SVD configurations
df_svd_400_10 <- calculate_overlap(rec_ibcf_400, rec_svd_400_10, 15, "SVD 10")
df_svd_400_20 <- calculate_overlap(rec_ibcf_400, rec_svd_400_20, 15, "SVD 20")
df_svd_400_30 <- calculate_overlap(rec_ibcf_400, rec_svd_400_30, 15, "SVD 30")
df_svd_400_40 <- calculate_overlap(rec_ibcf_400, rec_svd_400_40, 15, "SVD 40")
df_svd_400_50 <- calculate_overlap(rec_ibcf_400, rec_svd_400_50, 15, "SVD 50")

# Bind the data frames together
different_SVD <- bind_rows(
  df_svd_400_10, df_svd_400_20, df_svd_400_30,
  df_svd_400_40, df_svd_400_50
)
```

Wir können an unserer Tabelle sehen, das bei der Intersektion sich zwischen den SVD Modellen sich nicht wirklich etwas verändert.
```{r}
different_SVD %>%
  group_by(model) %>%
  summarise(mean(val))
```

Nun plotten wir das ganze noch, um es visuell darzustellen.
Es sieht bei allen ziemlich ähnlich aus. Es gibt fast keine Unterschiede.
```{r}
ggplot() +
  geom_histogram(data = different_SVD, aes(val, fill = model), binwidth = 1) +
  facet_wrap(. ~ model) +
  labs(
    x = "Intersections in %",
    y = "#Users",
    title = "Intersections of IBCF and various SVD"
  )
```

# 6.8

Wir erstellen eine coverage Funktion.

Die Coverage-Funktion  misst, welchen Anteil der verfügbaren Elemente durch die Filme des Systems abgedeckt wird, und teilt sie durch die Menge aller Filme

first we need the data
```{r}
# data <- movie_data
```

```{r}
cover <- function(N, data, rectype) {
  list_rec <- as(predict(rectype, data, n = N), "list") # right prob
  unl <- unlist(unlist(names(list_rec)) %>% map(~ (list_rec[[.x]])))
  length <- length(unique(movie_data$item))

  length(unique(unl)) / length
}
```


Jetzt machen wir die Novelty funktion:


Die novelty Funktion berechnet die "Novelty" (Neuheit) eines Empfehlungssystems. Novelty misst, wie überraschend oder neu die Empfehlungen für einen Nutzer sind. 


## second novelty try
```{r}
novelty <- function(N, data, rectype) {
  rating_n <- nrow(merged_data %>% group_by(item) %>% count())
  popularity <- merged_data %>%
    group_by(item) %>%
    count()
  popularity$n <- popularity$n / rating_n
  total <- list(0)
  recomlist <- as(predict(rectype, data, n = N), "list")

  for (i in 1:nrow(data)) {
    tmp <- merge(x = recomlist[[i]], y = popularity) %>% filter(x == item)
    sum <- -sum(log2(tmp$n)) / N
    total <- rbind(total, sum)
  }

  Reduce("+", total) / length(recomlist)
}
```


Plotten für data_400 mithilfe von IBCF

Zuerst erstellen wir die Metriken.
```{r}
# ibcf_400 was defined earlier, so it commented out
ibcf_400 <- Recommender(trainData1, method = "IBCF", param = list(
  method = "Cosine",
  k = 30
))

cvals400 <- c()
nvals400 <- c()

for (obs in seq(5, 30, 5)) {
  cv <- cover(obs, testData1, ibcf_400)
  nv <- novelty(obs, testData1, ibcf_400)

  # add it to list
  cvals400 <- rbind(cvals400, cv)
  nvals400 <- rbind(nvals400, nv)
}
```




now we create the plot
```{r}
metrics <- cbind(as.data.frame(cvals400), as.data.frame(nvals400))

colnames(metrics)[1] <- "coverage"
colnames(metrics)[2] <- "Novelty"
```


```{r}
metrics$xvalues <- c(5, 10, 15, 20, 25, 30)
metrics
```
Wir plotten es.


```{r}
# Create a scatterplot with different colors for N values
ggplot(metrics, aes(x = Novelty, y = coverage, color = factor(xvalues))) +
  geom_point(size = 3) +
  labs(
    title = "Scatterplot of Novelty vs Coverage",
    x = "Novelty",
    y = "Coverage",
    color = "N Values"
  ) +
  scale_color_manual(values = c("5" = "blue", "10" = "green", "15" = "red", "20" = "purple", "25" = "orange", "30" = "pink")) +
  theme_minimal()
```

Wir sehen, wie novelty mit höhere Anzahl vorgeschlagener Filme ansteigt und dann wieder absinkt, und wir sehen, 
dass die coverage stetig steigt.


Nun machen wir noch dasselbe für svd

```{r}
ibcf_400 <- Recommender(trainData1, method = "IBCF", param = list(
  method = "Cosine",
  k = 30
))
svd_400 <- Recommender(trainData1, method = "SVD", param = list(k = 30))


cvals400 <- c()
nvals400 <- c()

for (obs in seq(5, 30, 5)) {
  cv <- cover(obs, testData1, svd_400)
  nv <- novelty(obs, testData1, svd_400)

  # add it to list
  cvals400 <- rbind(cvals400, cv)
  nvals400 <- rbind(nvals400, nv)
}


smetrics <- cbind(as.data.frame(cvals400), as.data.frame(nvals400))

colnames(smetrics)[1] <- "coverage"
colnames(smetrics)[2] <- "Novelty"

smetrics$xvalues <- c(5, 10, 15, 20, 25, 30)
smetrics
```

Nun plotten wir die System-Metriken als Scatterplot “Novelty vs Coverage” für Top-N Listen der Länge N = 5,10, 15, 20, 25, 30 für truncated svd

```{r}
# Create a scatterplot with different colors for N values
ggplot(smetrics, aes(x = Novelty, y = coverage, color = factor(xvalues))) +
  geom_point(size = 3) +
  labs(
    title = "Scatterplot of Novelty vs Coverage (SVD)",
    x = "Novelty",
    y = "Coverage",
    color = "N Values"
  ) +
  scale_color_manual(values = c("5" = "blue", "10" = "green", "15" = "red", "20" = "purple", "25" = "orange", "30" = "pink")) +
  theme_minimal()
```

Hier sehen wir, dass sowohl Novelty als auch Coverage mit mehr Empfehlungen stetig zunehmen, anders als bei IBCF

## tbd nun für data_600

```{r}

```



# 6.9 new try
Als erstes bestimmen wir unser good Rating

Wir suchen ein ausgewogenes Verhältnis zwischen "guten" und "schlechten" Ratings wünschst und eine ähnliche Sensitivität und Spezifität.
Deswegen wählen wir 0.5 als quantil
Dies ist unser good Rating.
```{r}
gR <- quantile(merged_data$rating, 0.5)
gR
```

## Modellwahlen
wir benutzen die Modelle
SVD
IBCF cos
UBCF cos
IBCF Pearson
UBCF Pearson

Erklärung:

Warum benutzen wir die Modelle?

SVD (Singular Value Decomposition): SVD ist eine weit verbreitete Matrixfaktorisierungstechnik, die gut auf latente Faktoren in den Daten reagiert. Es ist bekannt für seine gute Leistung bei der Entdeckung von Muster in großen Datensätzen.

IBCF (Item-Based Collaborative Filtering) mit Cosine-Ähnlichkeit: IBCF basiert auf der Ähnlichkeit zwischen Elementen und eignet sich gut für Datensätze mit klaren Elementähnlichkeiten. Cosine-Ähnlichkeit ist eine einfache und effektive Methode, um die Ähnlichkeit zwischen Elementen zu messen.

UBCF (User-Based Collaborative Filtering) mit Cosine-Ähnlichkeit: UBCF betrachtet die Ähnlichkeit zwischen Benutzern. Cosine-Ähnlichkeit wird auch hier verwendet, um die Ähnlichkeit zwischen Benutzerprofilen zu berechnen.

IBCF mit Pearson-Ähnlichkeit: Statt Cosine-Ähnlichkeit kann auch Pearson-Ähnlichkeit für IBCF verwendet werden. Pearson-Ähnlichkeit berücksichtigt die Zentrierung der Daten und kann in bestimmten Szenarien bevorzugt werden.

UBCF mit Pearson-Ähnlichkeit: Analog zu IBCF, aber für Benutzer statt Elemente.



```{r}
top_400_scheme <- evaluationScheme(rrm_400,
  goodRating = gR,
  method = "cross-validation",
  k = 10, given = 20
)

approaches <- list(
  "Popular movies" = list(name = "POPULAR", param = NULL),
  "SVD" = list(name = "SVD", param = list(k = 10)),
  "IBCF cosinus" = list(
    name = "IBCF",
    param = list(method = "Cosine", k = 10)
  ),
  "IBCF Pearson" = list(
    name = "IBCF",
    param = list(method = "Pearson", k = 10)
  ),
  "UBCF cosinus" = list(
    name = "UBCF",
    param = list(method = "Cosine", k = 10)
  ),
  "UBCF Pearson" = list(
    name = "UBCF",
    param = list(method = "Pearson", k = 10)
  )
)

steps <- c(10, 15, 20, 25, 30)
```

Jetzt plotten wir die 5 Modelle für data_400

Als Metriken benutzen wir True Positive Rate (TPR) und False Positive Rate (FPR:

True Positive Rate (TPR):
Anteil der tatsächlichen positiven Instanzen, die vom Modell korrekt als positiv vorhergesagt wurden.
  Formel: TPR = TP / (TP + FN)
  TP (True Positive): Anzahl der korrekt als positiv vorhergesagten Instanzen.
  FN (False Negative): Anzahl der falsch als negativ vorhergesagten Instanzen.
  
  
False Positive Rate (FPR):
FPR ist der Anteil der tatsächlich negativen Instanzen, die vom Modell fälschlicherweise als positiv vorhergesagt wurden.
  Formel: FPR = FP / (FP + TN)
  FP (False Positive): Anzahl der falsch als positiv vorhergesagten Instanzen.
  TN (True Negative): Anzahl der korrekt als negativ vorhergesagten Instanzen.
```{r}
comparemod400 <- evaluate(top_400_scheme, approaches,
  n = steps, type = "topNList",
  progress = FALSE
)

plot(comparemod400, avg = TRUE, lty = 1, annotate = 1, legend = "topleft")
```

Unser SVD Modell ist klar das beste Modell. Doch vielleicht lässt sich das SVD Modell
noch mithilfe von Hyperparameter optimieren.

Der Standardwert von gamna is 0.015
Der Standardwert von lambda ist 0.001(https://rdrr.io/cran/recommenderlab/src/R/RECOM_SVDF.R)

Der Standardwert von lambda ist 0.001

## 6.9.4 Hyperparameter
Experimentieren mit gamna:
```{r}
top_400_scheme <- evaluationScheme(rrm_400,
  goodRating = gR,
  method = "cross-validation",
  k = 10, given = 20
)
approaches <- list(
  "Popular movies" = list(name = "POPULAR", param = NULL),
  "SVD gamma 0.005" = list(
    name = "SVD",
    param = list(k = 10, gamma = 0.005)
  ),
  "SVD gamma 0.010" = list(
    name = "SVD",
    param = list(k = 10, gamma = 0.010)
  ),
  "SVD gamma def(0.015)" = list(
    name = "SVD",
    param = list(k = 10, gamma = 0.015)
  ),
  "SVD gamma 0.020" = list(
    name = "SVD",
    param = list(k = 10, gamma = 0.020)
  ),
  "SVD gamma 0.025" = list(
    name = "SVD",
    param = list(k = 10, gamma = 0.025)
  ),
  "SVD gamma 0.5" = list(
    name = "SVD",
    param = list(k = 10, gamma = 0.5)
  )
)

comparemod400 <- evaluate(top_400_scheme, approaches, n = steps, type = "topNList", progress = FALSE)

plot(comparemod400, avg = TRUE, lty = 1, annotate = 1, legend = "topleft")
```
Alle gamma scheinen aufeinander zu liegen. Der hyperparameter gamma scheint nicht wichtig zu sein.


Experimentieren mit lambda:
```{r}
top_400_scheme <- evaluationScheme(rrm_400,
  goodRating = gR,
  method = "cross-validation",
  k = 10, given = 20
)

approaches <- list(
  "Popular movies" = list(name = "POPULAR", param = NULL),
  "SVD lambda 0.0001" = list(
    name = "SVD",
    param = list(k = 10, lambda = 0.0001)
  ),
  "SVD lambda def(0.001)" = list(
    name = "SVD",
    param = list(k = 10, lambda = 0.001)
  ),
  "SVD lambda 0.002" = list(
    name = "SVD",
    param = list(k = 10, lambda = 0.002)
  ),
  "SVD lambda 0.005" = list(
    name = "SVD",
    param = list(k = 10, lambda = 0.005)
  ),
  "SVD lambda 0.01" = list(
    name = "SVD",
    param = list(k = 10, lambda = 0.01)
  ),
  "SVD lambda 0.02" = list(
    name = "SVD",
    param = list(k = 10, lambda = 0.02)
  )
)

comparemod400 <- evaluate(top_400_scheme, approaches, n = steps, type = "topNList", progress = FALSE)

plot(comparemod400, avg = TRUE, lty = 1, annotate = 1, legend = "topleft")
```

Wir sehen wieder keinen Unterschied, egal wie sehr wir die Parameter anpassen.